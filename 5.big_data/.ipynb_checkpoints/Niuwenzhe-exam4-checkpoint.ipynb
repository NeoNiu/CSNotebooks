{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习集训营六期第四周(Hadoop&Spark)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年11月16日至11月18日期间完成，最晚提交时间本周日（11月18日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试提交方式：请同学<font color=red><b>拷贝</b></font>该试卷后，将文件更名为同学姓名拼音-exam4（例如wangwei-exam4）后<font color=red><b>移动</b></font>至/0.Teacher/Exam/4/目录下进行作答。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分处不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名: 牛文哲\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共4题，每题10分，共计40分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.简述Hadoop的优点有哪些？Spark与之相比又有哪些优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hadoop\n",
    "HDFS(Hadoop Distributed File System) 是一个分布式存储和计算平台。\n",
    "1. 低成本。将大文件分割成许多小的数据块，分别存储在不同的服务器结点上,对服务器要求不高，可以运行在普通PC服务器组成的集群中。\n",
    "2. 可靠性，即容错性高。通过分布式存储，hadoop可以自动存储多份副本，当数据处理请求失败后，会自动重新部署计算任务。\n",
    "3. 高效性。因为是分布式存储和计算，接受到客户的数据请求后，hadoop可以在数据所在的集群节点上并发处理。\n",
    "4. 可扩展性。hadoop的分布式存储和分布式计算是在集群节点完成的，这也决定了hadoop可以扩展至更多的集群节点。\n",
    "\n",
    "##### Spark\n",
    "Spark的优点即解决了Hadoop的缺点。Hadoop的缺点有什么呢？最大的问题应该是高IO：Hadoop是基于流处理的，Hadoop会从(物理存储)hdfs中加载数据，然后处理之后再返回给物理存储hdfs中，这样不断的读取与写入，占用了大量的IO。所以在机器学习，图计算，数据挖掘方面不适用，现在要做大量的重复操作，并且下一次的开始，要依据前面计算的结果，这样对于hadoop来说就要重新的计算，从而浪费大量的资源。\n",
    "\n",
    "Spark提出了分布式的内存抽象：\n",
    "1. 运算速度快: 与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上。\n",
    "2. 容易使用: 通过建立在Java,Scala,Python,SQL（应对交互式查询）的标准API以方便各行各业使用，同时还含有大量开箱即用的机器学习库。\n",
    "3. 可迁移性: 与现有Hadoop 1和2.x(YARN)生态兼容，因此机构可以无缝迁移。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.请简述您对MAP-REDUCE这一编程模型的理解 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T08:13:01.601778Z",
     "start_time": "2018-11-17T08:13:01.591272Z"
    }
   },
   "source": [
    " MapReduce的基本原理就是：将大的数据分析分成小块逐个分析，最后再将提取出来的数据汇总分析，最终获得我们想要的内容。具体而言分为：\n",
    "    1. 用户程序首先调用的MapReduce库将输入文件分成M个数据片度。然后用户程序在集群中创建大量的程序副本。\n",
    "    2. 被分配了Map任务的服务器结点读取相关的输入数据片段，从输入的数据片段中解析出key/value对，然后把key/value对传递给用户自定义的Map函数，由Map函数生成并输出的中间key/value对，并缓存在内存中。\n",
    "    3. Map阶段运行完毕，对产生的key/value对进行排序操作。\n",
    "    4. Reduce程序遍历排序后的中间数据，对于每一个唯一的中间key值，Reduce程序将这个key值和它相关的中间value值的集合传递给用户自定义的Reduce函数。Reduce函数的输出被追加到所属分区的输出文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请简述RDD的含义，并写出针对RDD的两类操作（transformation与action),每类下至少三种的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Resilient Distributed Dataset(弹性分布式数据集，简称RDD)，是spark的基本数据结构，spark中的所有数据都是通过RDD的形式进行组织。\n",
    " - transformation 操作\n",
    "* `map()` 对RDD的每一个item都执行同一个操作\n",
    "* `flatMap()` 对RDD中的item执行同一个操作以后得到一个list，然后以平铺的方式把这些list里所有的结果组成新的list\n",
    "* `filter()` 筛选出来满足条件的item\n",
    "* `distinct()` 对RDD中的item去重\n",
    "* `sample()` 从RDD中的item中采样一部分出来，有放回或者无放回\n",
    "* `sortBy()` 对RDD中的item进行排序\n",
    " - action 操作\n",
    "* `collect()`: 计算所有的items并返回所有的结果到driver端，接着 `collect()`会以Python list的形式返回结果\n",
    "* `first()`: 和上面是类似的，不过只返回第1个item\n",
    "* `take(n)`: 类似，但是返回n个item\n",
    "* `count()`: 计算RDD中item的个数\n",
    "* `top(n)`: 返回头n个items，按照自然结果排序\n",
    "* `reduce()`: 对RDD中的items做聚合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Spark内置了机器学习库mllib，请写出使用该库完成一项机器学习任务的通用步骤\n",
    "- （注意：仅步骤即可，可GOOGLE）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-17T08:29:40.872806Z",
     "start_time": "2018-11-17T08:29:40.845063Z"
    }
   },
   "outputs": [],
   "source": [
    "#### 以二分类问题为例  来自mllib官网\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "\n",
    "# 导入数据集\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"data/mllib/sample_svm_data.txt\")\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# 使用逻辑回归训练模型\n",
    "model = LogisticRegressionWithSGD.train(parsedData)\n",
    "\n",
    "# 使用训练好的模型评估测试集\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验题(共2题，共计60分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.HDFS实验题(25分，共5个小题，每个小题5分)\n",
    "#### 请写出完成以下任务的HDFS对应的文件(夹)操作命令\n",
    "- 1.在hdfs根目录下新建/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs fs -mkdir /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2.把本地文件test.txt test2.txt放入该文件夹（注，test.txt与test2.txt可以是同学自建的空文件）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#当前位置在根目录下\n",
    "hdfs fs -put test.txt /sxy-new/\n",
    "hdfs fs -put test2.txt /sxy-new/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.从hdfs上取下文件old.txt(假定在/sxy-new下有该文件)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#存在本地当前目录\n",
    "hdfs fs -get /sxy-new/old.txt /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 4.从hdfs上取下/sxy-new中所有内容，并合成一个本地文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -getmerge /sxy-new/* new_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5.把/sxy-new拷贝到/tmp下后，删除/sxy-new文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop fs -cp /sxy-new /tmp/\n",
    "hadoop fs -rm -r /sxy-new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.PySpark实验题(35分)\n",
    "请把0.Teacher/Data/data.csv文件放置在服务器上后，在服务器上启动pyspark命令（tip:可以使用scp命令进行文件传输）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.载入数据与了解基本信息   (共10分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  通过命令统计raw_content中的记录条数 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#配置sparkcontext\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "import os\n",
    "conf=SparkConf().setAppName(\"exam\").setMaster(\"local[*]\")\n",
    "sc=SparkContext.getOrCreate(conf)\n",
    "cwd=os.getcwd()\n",
    "#读入本地文件\n",
    "content_rdd=sc.textFile('file://'+cwd+'/data.csv').map(lambda x:x.split(','))\n",
    "\n",
    "#统计文件\n",
    "content_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从raw_content这个RDD中取出前5条记录输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从raw_content中采样出3条记录输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_rdd.takeSample(False,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：你将看到类似如下的结果：\n",
    "```\n",
    "[\n",
    "[u'\"date\"', u'\"time\"', u'\"size\"', u'\"r_version\"', u'\"r_arch\"', u'\"r_os\"', u'\"package\"', u'\"version\"', u'\"country\"', u'\"ip_id\"'], \n",
    "[u'\"2015-12-12\"', u'\"13:42:10\"', u'257886', u'\"3.2.2\"', u'\"i386\"', u'\"mingw32\"', u'\"HistData\"', u'\"0.7-6\"', u'\"CZ\"', u'1'], \n",
    "[u'\"2015-12-12\"', u'\"13:24:37\"', u'1236751', u'\"3.2.2\"', u'\"x86_64\"', u'\"mingw32\"', u'\"RJSONIO\"', u'\"1.3-0\"', u'\"DE\"', u'2']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.数据处理1   (5分)\n",
    "- 对以上数据进行清洗，把记录中带的引号去除掉，得到content_rdd\n",
    "- tip：参考map函数的用法\n",
    "\n",
    "你将得到类似以下结果的RDD：\n",
    "```\n",
    "[\n",
    "[u'date', u'time', u'size', u'r_version', u'r_arch', u'r_os', u'package', u'version', u'country', u'ip_id'], \n",
    "[u'2015-12-12', u'13:42:10', u'257886', u'3.2.2', u'i386', u'mingw32', u'HistData', u'0.7-6', u'CZ', u'1'], \n",
    "[u'2015-12-12', u'13:24:37', u'1236751', u'3.2.2', u'x86_64', u'mingw32', u'RJSONIO', u'1.3-0', u'DE', u'2'], \n",
    "[u'2015-12-12', u'13:42:35', u'2077876', u'3.2.2', u'i386', u'mingw32', u'UsingR', u'2.0-5', u'CZ', u'1']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_rdd.map(lambda x:[i[1:-1] for i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 数据处理2（5分)\n",
    "- 有如下的text，请使用flatmap得到包含所有字幕的list\n",
    "- tip：请参考课程flatmap的使用，特别注意这里不同分割符的处理\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "\n",
    "你将得到如下结果的RDD：\n",
    "```\n",
    "[a, b, c, d, e, f, g, h, m, n, q, r, q, w, j, r, q]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text=[\"a b c\", \"d e\", \"f#g#h\", \"m n q\", \"r,q,w\", \"j%r%q\"]\n",
    "text_rdd = sc.parallelize(text)\n",
    "text_rdd = text_rdd.flatMap(lambda x:re.split(r\"\\s|,|\\#|\\%\",x))\n",
    "list_get=text_rdd.collect()\n",
    "list_get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 数据处理3（5分）\n",
    "- 从刚才的content_rdd中取出第7列，对不同的package类别进行统计计数\n",
    "- tip：可以使用map reduce或者pair-rdd reduceByKey\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd = content_rdd.map(lambda word_list: word_list[6])\n",
    "                .map(lambda word: (word,1))\n",
    "                .reduceByKey(lambda x,y:x+y)\n",
    "resutl.rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.排序   (5分)\n",
    "- 取出数量最多的10个package和出现的频次\n",
    "- tip：注意sortbykey的使用\n",
    "\n",
    "你将看到类似如下的结果：\n",
    "```\n",
    "[(4783, u'Rcpp'),\n",
    " (3913, u'ggplot2'),\n",
    " (3748, u'stringi'),\n",
    " (3449, u'stringr'),\n",
    " (3436, u'plyr'),\n",
    " (3265, u'magrittr'),\n",
    " (3223, u'digest'),\n",
    " (3205, u'reshape2'),\n",
    " (3046, u'RColorBrewer'),\n",
    " (3007, u'scales')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_rdd = result_rdd.map(lambda x: (x[1],x[0]))\n",
    "                        .sortByKey(ascending = False)\n",
    "sorted_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.过滤  (5分)\n",
    "\n",
    "- 取出top5的package对应的数据记录\n",
    "- tip：注意filter的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_rdd = sorted_rdd.map(lambda x: x[1]).take(5)\n",
    "filtered_rdd = content_rdd.filter(lambda x: x[6] in top5_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
