{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七月在线机器学习集训营六期第五周(机器学习基础)考试\n",
    "#### 考试说明:\n",
    "- 起止时间：请同学在2018年11月23日至11月25日期间完成，最晚提交时间本周日（11月25日24时之前）结束，<b>逾期不接受补考,该考试分数计入平时成绩</b>\n",
    "- 考试提交方式：请同学<font color=red><b>拷贝</b></font>该试卷后，将文件更名为同学姓名拼音-exam4（例如wangwei-exam4）后<font color=red><b>移动</b></font>至/0.Teacher/Exam/4/目录下进行作答。\n",
    "- 注意事项：为确保同学们真正了解自身对本周课程的掌握程度，<font color=red><b>请勿翻阅，移动，更改</b></font>其它同学试卷。如发现按0分处理\n",
    "- 请同学在下方同学姓名处填写自己的姓名，批改人和最终得分处不用填写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 同学姓名: 牛文哲\n",
    "- 批改人：   \n",
    "- 最终得分:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>####答卷开始####</h1></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问答题(共10题，每题10分，共计100分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.请分别解释统计机器学习中的输入/输出空间，特征空间，假设空间与参数空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 输入/输出空间\n",
    "所有输入样本X的所有取值称为输入空间；所有输出样本Y的所有取值称为输出空间。输入/输出空间可以是有限集合空间，也可以是整个欧式空间。通常输出空间远远小于输入空间。\n",
    "- 特征空间\n",
    "每一个具体的输入是一个实例，由特征向量表示。所有特征向量存在的空间，称为特征空间。\n",
    "- 假设空间\n",
    "监督学习中，学习的目的是找到一个由输入到输出的映射，由模型来表示。模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间。\n",
    "- 参数空间\n",
    "上述模型中，模型参数的可能取值称为参数空间。\n",
    "\n",
    "-- 以西瓜数据集为例说明：\n",
    "西瓜的所有属性（色泽、敲声）的取值（乌黑、沉闷）组成的空间为输入空间。\n",
    "西瓜的好坏称为输出空间。\n",
    "西瓜的所有属性存在的空间为特征空间。\n",
    "可以对数据集训练出不同的由输入空间映射到输出空间的模型，这些可能的模型所在的空间即假设空间。\n",
    "对与训练的模型，参数的可能取值称为空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.什么是损失函数，并举例有哪些常用的损失函数？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数（Loss function）是用来估量模型的预测值 f(X) 与真实值 Y 的不一致程度的一个非负实值函数。 损失函数越小，模型就越好。\n",
    "\n",
    "常用的损失函数有：\n",
    "- 平方损失函数（MSE）\n",
    "- 交叉熵损失函数（Cross Entropy）\n",
    "- 合页损失函数（hinge loss)\n",
    "- 绝对值损失函数（MAE）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请结合公式进行说明结构风险最小化的含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结构风险的公式定义为：\n",
    "$$ R_{srm}= \\frac{1}{N} \\sum_{i=1}^{N}   L \\left ( y_{i},f\\left ( x_{i} \\right ) \\right ) + \\lambda J(f) $$\n",
    "其中$\\frac{1}{N} \\sum_{i=1}^{N}   L \\left ( y_{i},f\\left ( x_{i} \\right ) \\right )$一项为模型的损失函数，即结构风险，后面的$ J(f) $ 为模型复杂度的正则化项。模型越复杂，正则化项越大。最小化二者的和，即需要损失函数和模型复杂度同时小，或者达到二者的最佳平衡点。结构风险小的模型往往泛化能力比较好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.谈谈什么是生成式模型与判别式模型，以及各自特点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成方法由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即生成模型。\n",
    "$$P(Y|X) = \\frac{P(X,Y)}{P(X)}$$\n",
    "判别方法由数据直接学习决策函数$f(X)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型。\n",
    "- 生成模型的特点：\n",
    "生成方法可以还原出联合概率分布，判别方法则不能。 收敛速度更快。\n",
    "- 判别模型的特点：\n",
    "学习准确率高，可以对数据进行各种程度上的抽象、定义特征，可以简化学习问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请解释范数，并写出L1,L2范数的定义公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "范数的公式定义为：\n",
    "$$ \\left \\| \\mathbf{X} \\right \\|_{p}=\\left({\\sum_{i=1}^{N}{|X_{i}|^{p}}}\\right) ^\\frac{1}{p} $$\n",
    "范数是一种数学上的概念，从宏观上来讲，范数是将向量映射到非负值的函数，从直观上来说，范数是向量X到原点的距离。在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即①非负性；②齐次性；③三角不等式。\n",
    "它常常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。\n",
    "- L1范数公式\n",
    "$$\\left \\| \\mathbf{X} \\right \\|_{1} =\\sum_{i=1}^{N} \\left | x_{i} \\right |$$\n",
    "- L2范数公式\n",
    "$$\\left \\| \\mathbf{X} \\right \\|_{2} =\\sum_{i=1}^{N} {x_{i}}^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 什么是信息熵，它的公式是？什么是信息增益?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息熵是度量样本集合纯度的一种指标。\n",
    "- 信息熵公式\n",
    "$$Ent(D) = - \\sum_{k=1}^{|y|}p_{k}log p_{k}$$\n",
    "- 信息增益 是根据某属性取值不同将样本分为不同的子集后，子集的加权信息熵之和 相对 与未划分子集之前的 信息熵的 下降量，用公式表示为：\n",
    "$$Gain(D,a) = Ent(D) - \\sum_{v=1}^{V} \\frac{\\left | D^{v} \\right |}{\\left | D \\right |} Ent(D^{v})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. 贝叶斯公式是什么？您如何理解贝叶斯思想？它与频率派有哪些区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 贝叶斯公式\n",
    "$$ P(Y|X)=\\frac{P(X|Y)P(Y)}{P(X)} $$ 或者：$$ P(B_{i}|A)=\\frac{P(B_{i})P(A|B_{i})}{\\sum_{j=1}^{n}{P(B_{j})P(A|B_{j})}}$$\n",
    "- 贝叶斯思想\n",
    "    - 贝叶斯思想不相信任何事件发生的背后都拥有一个固定不变的分布，其更倾向于认为世界上所有的事情都是不确定的，而这种不确定型更多是由于观察者自身所储备的先验知识所带来的。未知的不确定性程度由先验概率分布和现象出现的概率分布决定。\n",
    "- 与频率派的区别\n",
    "    - 贝叶斯学派重视先验，频率学派重视似然。具体来说，就是贝叶斯学派认为参数是变量，而频率学派认为参数虽然未知，但确实客观存在的固定值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.请从最大似然的角度去解释逻辑回归,以及逻辑回归的损失函数是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最大似然法 即 给定一个模型，优化这个模型中的参数，最大化似然函数，使这个模型更好的匹配现有样本(如果不考虑模型的泛化能力)。\n",
    "- 对于二项逻辑回归，其模型为符合以下条件概率分布\n",
    "$$ P(Y=1|\\mathit{x}) = \\frac{1}{1+e^{-(w\\cdot x+b)}} $$\n",
    "$$ P(Y=0|\\mathit{x}) = 1- \\frac{1}{1+e^{-(w\\cdot x+b)}}$$\n",
    "令$$ g(x) = \\frac{1}{1+e^{-(w\\cdot x+b)}}$$\n",
    "则似然函数为：\n",
    "$$ \\prod_{i=1}^{N}\\left [ g(x_{i}) \\right ]^{y_{i}}\\left [1- g(x_{i}) \\right ]^{1-y_{i}} $$\n",
    "对上式求对数获得 对数似然函数为：\n",
    "$$ L(\\theta ) = \\sum_{i=1}^{N}  \\left [ y_{i}log^{g(x_{i})} + (1-y_{i})log^{1-g(x_{i})} \\right] $$\n",
    "最大对数似然函数等价于最小化负对数似然函数，即得二项逻辑回归的损失函数：\n",
    "$$ J(\\theta)= -L(\\theta ) = -\\sum_{i=1}^{N}  \\left [ y_{i}log^{g(x_{i})} + (1-y_{i})log^{1-g(x_{i})} \\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.试析Min-Max与Z-Score这两种数据缩放各自特点，和为什么树形结构不需要做缩放？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Min-Max 缩放\n",
    "数据处理公式为：\n",
    "$$x_{i\\_new} = \\frac{x_{i}-x_{min}}{x_{max}-x_{min}}$$\n",
    "根据公式可知，所有数据值都讲被缩放到[0,1]的区间内，最小值映射为0，最大值映射为1。\n",
    "- Z-Score 缩放\n",
    "数据处理公式为：\n",
    "$$x _ { i\\_new } = \\frac { x_{i}  - \\overline { x } } { std }$$\n",
    "数据标准化之后被归纳成了一个均值为0，标准差为1的值。\n",
    "- 为什么树形结构不需要缩放\n",
    "    - 数据缩放是一般是对于连续型属性而做的预处理。 树形模型是规则型模型而不是计算型模型，以CART模型为例，其对于连续型变量的分类规则是，大于某值分为一类，数据缩放对于其分类结果没有影响。同时相对于计算模型而言，树模型没有梯度的计算，因此不同特征数据尺度的差异不会影响到树模型对于特征的选择和分裂点的选取。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. 请解释下随机森林和Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机森林和Xgboost是分别是两类集成模型 bagging 和 boosting 的代表算法，都是基于决策树这种弱学习器而获得的强学习器模型。\n",
    "- 随机森林\n",
    "    - 多颗决策树并行计算，每颗决策树从样本中随机抽取不同的样本（有放回）和不同的特征（可选）进行学习，最后通过对多颗决策树的决策结果取平均值或者使用投票法确定最终预测结果。因为每颗树都对样本进行了采样，所以能够较好的避免过拟合。\n",
    "- Xgboost\n",
    "    - 多颗决策树串行计算，第n颗树的生长基于第n-1颗树的预测结果的残差。 xgboost在目标函数中显示的加上了正则化项，基学习为CART时，正则化项可以是树的叶子节点的数量T等与树的复杂度有关的值。xgboost的损失损失函数为 \n",
    "    $$\\sum_i^n L(y_i,f^{(t-1)}(x_i)+T(x_i))+\\gamma|t|+\\lambda\\frac{1}{2}||w||^2$$\n",
    "GBDT中使用Loss Function对f(x)的一阶导数计算出伪残差用于学习生成fm(x)，xgboost不仅使用到了一阶导数，还使用二阶导数。\n",
    "第n次的loss：\n",
    "$$L_n\\approx \\sum_i^n L(y_i,f^{(n-1)}(x_i)+g^t_i*T(x_i)+\\frac{1}{2}h_iT(x_i)^2)+\\gamma|t|+\\lambda\\frac{1}{2}||w||^2$$\n",
    "其中：g为一阶导数，h为二阶导数\n",
    "\n",
    "二者在单颗树的生长时，特征点（分裂点）的选择计算中，都可以并行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><h1>####答卷结束####</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 本周课程意见反馈(非必答)\n",
    "请同学围绕以下两点进行回答：\n",
    "- 自身总结：您自己在本周课程的学习，收获，技能掌握等方面进行总结，包括自身在哪些方面存在哪些不足，欠缺，困惑。作为将来回顾学习路径时的依据。\n",
    "- 课程反馈：也可以就知识点，进度，难易度，教学方式，考试方式等等进行意见反馈，督促我们进行更有效的改进，为大家提供更优质的服务。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
